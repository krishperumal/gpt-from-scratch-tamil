{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d3bBBPnDSPJ",
        "outputId": "62b718f4-b9dd-49f5-a3aa-ffe550923b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "vocab_size=130\n",
            "chars=['\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'G', 'I', 'J', 'K', 'L', 'M', 'N', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', '©', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'க', 'ங', 'ச', 'ஜ', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ஷ', 'ஸ', 'ஹ', 'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ொ', 'ோ', '்', '௦', '௧', '௩', '௫', '௬', '௱', '—', '‘', '“', '”']\n",
            "encoded_text=[105, 93, 87, 119, 87, 98, 119, 10, 1, 82, 97, 119, 97, 92, 110, 1, 78, 100, 112, 87, 119, 87, 110, 101, 111, 100, 119, 87, 103, 119, 28]\n",
            "decoded_text='வணக்கம், எப்படி இருக்கிறீர்கள்?'\n",
            "len(train)=3526403\n",
            "len(val)=391823\n"
          ]
        }
      ],
      "source": [
        "# Read input data\n",
        "# Only for the first time - Download ponniyin selvan text as html\n",
        "# !wget -O ponniyin-selvan.html https://archive.org/stream/PonniyinSelvan/Ponniyin-Selvan_djvu.txt\n",
        "\n",
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# import shutil\n",
        "# shutil.copy('ponniyin-selvan.html', '/content/drive/MyDrive/Colab Notebooks/data')\n",
        "# Extract text from html file if it doesn't exist\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "html_file_name = '/content/drive/MyDrive/Colab Notebooks/data/ponniyin-selvan.html'\n",
        "text_file_name = '/content/drive/MyDrive/Colab Notebooks/data/ponniyin-selvan.txt'\n",
        "if not os.path.exists(text_file_name):\n",
        "\n",
        "  # Read the content of the downloaded file\n",
        "  with open(html_file_name, 'r') as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "  # Find the element with id=\"maincontent\"\n",
        "  main_content = soup.find('main', id='maincontent').find('pre')\n",
        "\n",
        "  # Print the extracted content\n",
        "  if main_content:\n",
        "    with open(text_file_name, 'w', encoding='utf-8', errors='ignore') as f:\n",
        "      f.write(main_content.get_text())\n",
        "    print(f\"Full text written to {text_file_name}\")\n",
        "  else:\n",
        "    print(\"Element with id='maincontent' not found.\")\n",
        "\n",
        "# read full file text\n",
        "with open(text_file_name, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "  input_text = f.read()\n",
        "  # clean up text\n",
        "  input_text = re.sub(r'\\u200c|Table of Contents', '', input_text).strip()\n",
        "\n",
        "# Get Vocabulary\n",
        "chars = sorted(list(set(input_text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"{vocab_size=}\")\n",
        "print(f\"{chars=}\")\n",
        "\n",
        "# Use character encodings\n",
        "stoi = { s:i for (i,s) in enumerate(chars)}\n",
        "itos = { i:s for (i,s) in enumerate(chars)}\n",
        "\n",
        "encode = lambda txt: [stoi[s] for s in txt]\n",
        "decode = lambda enc: ''.join([itos[i] for i in enc])\n",
        "\n",
        "encoded_text = encode('வணக்கம், எப்படி இருக்கிறீர்கள்?')\n",
        "print(f\"{encoded_text=}\")\n",
        "decoded_text = decode(encoded_text)\n",
        "print(f\"{decoded_text=}\")\n",
        "\n",
        "# train and val split\n",
        "import torch\n",
        "split_idx = int(0.9*len(input_text))\n",
        "train = torch.tensor(encode(input_text[:split_idx]))\n",
        "val = torch.tensor(encode(input_text[split_idx:]))\n",
        "print(f\"{len(train)=}\")\n",
        "print(f\"{len(val)=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jM7VCrnDDkR0"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# seed for reproducibility\n",
        "torch.manual_seed(9871)\n",
        "\n",
        "# Hyperparameters\n",
        "block_size = 256\n",
        "batch_size = 64\n",
        "embedding_dim = 384\n",
        "head_size = 64\n",
        "num_heads = 6\n",
        "num_decoder_layers = 6\n",
        "dropout = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NsU8vGXMERre"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "  if split == 'train':\n",
        "    data = train\n",
        "  else:\n",
        "    data = val\n",
        "\n",
        "  # get random batch indexes\n",
        "  idxs = torch.randint(0, data.size(0)-block_size, (batch_size,))\n",
        "  x = torch.stack([data[idx:idx+block_size] for idx in idxs])\n",
        "  y = torch.stack([data[idx+1:idx+block_size+1] for idx in idxs])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2TkIN9eFp5k",
        "outputId": "66316afd-ed88-4d43-dab1-19de97345caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits=tensor([[ 0.2466, -0.0025, -0.3208,  ..., -0.4202,  0.6906,  0.8823],\n",
            "        [ 0.4852,  1.2482, -0.6324,  ..., -0.3995, -0.0739,  1.1016],\n",
            "        [ 0.5163, -0.1241, -0.7091,  ...,  0.5189,  0.2061, -0.2577],\n",
            "        ...,\n",
            "        [-0.3484, -0.6811, -0.2737,  ...,  0.7043,  0.0604,  0.5762],\n",
            "        [-0.5807,  0.1280,  0.1906,  ...,  0.4386,  0.1946, -0.1571],\n",
            "        [ 0.1031,  0.5801, -1.0247,  ..., -1.1586,  0.2030,  0.8819]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "loss=tensor(4.9634, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "    self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "    self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # we require T for initializing generation from potentially less than\n",
        "    # block size number of tokens as input\n",
        "    _, T, _ = x.shape\n",
        "    q = self.key(x)\n",
        "    k = self.query(x)\n",
        "    v = self.value(x)\n",
        "    weights = q @ k.transpose(-2, -1) * head_size**-2\n",
        "    weights = weights.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    weights = F.softmax(weights, dim=-1)\n",
        "    weights = self.dropout(weights)\n",
        "    output = weights @ v\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.attention_heads = nn.ModuleList([\n",
        "        AttentionBlock() for _ in range(num_heads)\n",
        "    ])\n",
        "    self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    att = torch.cat([att(x) for att in self.attention_heads], dim=-1)\n",
        "    lin = self.linear(att)\n",
        "    out = self.dropout(lin)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ffwd = nn.Sequential(\n",
        "      nn.Linear(embedding_dim, embedding_dim*4),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(embedding_dim*4, embedding_dim),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.ffwd(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.multi_head_attention = MultiHeadAttention()\n",
        "    self.feed_forward = FeedForward()\n",
        "    self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "    self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    att = self.multi_head_attention(x)\n",
        "    att_addnorm = self.layer_norm1(x + att)\n",
        "    ffwd = self.feed_forward(att)\n",
        "    ffwd_addnorm = self.layer_norm2(x + ffwd)\n",
        "    return ffwd_addnorm\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(\n",
        "        num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "    self.pos_embedding = nn.Embedding(\n",
        "        num_embeddings=block_size, embedding_dim=embedding_dim)\n",
        "    self.decoder_blocks = nn.Sequential(*[DecoderBlock()\n",
        "        for _ in range(num_decoder_layers)])\n",
        "    self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    # assuming that only last block_size elements are sent as input\n",
        "    B, T = x.shape\n",
        "    emb = self.token_embedding(x) + self.pos_embedding(torch.arange(T, device=device))\n",
        "    decoded = self.decoder_blocks(emb)\n",
        "    logits = self.linear(decoded)\n",
        "\n",
        "    if y is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      y = y.view(B*T)\n",
        "      loss = F.cross_entropy(logits, y)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, x, max_num_steps=1000):\n",
        "    for _ in range(max_num_steps):\n",
        "      logits, _ = self(x[:, :block_size])\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      x_next = torch.multinomial(probs, num_samples=1)\n",
        "      x = torch.cat((x, x_next), dim=1)\n",
        "    return x\n",
        "\n",
        "# test forward with random batch\n",
        "xb, yb = get_batch('train')\n",
        "model = Transformer()\n",
        "model = model.to(device)\n",
        "logits, loss = model(xb, yb)\n",
        "print(f\"{logits=}\")\n",
        "print(f\"{loss=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USnrdp3qerN-",
        "outputId": "4ad3dbfc-a453-4224-c2a5-692425e93b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 3.911546230316162 at i=10\n",
            "Validation 3.3674254417419434 at i=10\n",
            "Train 3.328821897506714 at i=20\n",
            "Validation 3.255491256713867 at i=20\n",
            "Train 3.2645983695983887 at i=30\n",
            "Validation 3.1883559226989746 at i=30\n",
            "Train 3.1958117485046387 at i=40\n",
            "Validation 3.159634828567505 at i=40\n",
            "Train 3.123216152191162 at i=50\n",
            "Validation 3.0529580116271973 at i=50\n",
            "Train 3.0370006561279297 at i=60\n",
            "Validation 2.963747978210449 at i=60\n",
            "Train 2.939958095550537 at i=70\n",
            "Validation 2.876465320587158 at i=70\n",
            "Train 2.839512348175049 at i=80\n",
            "Validation 2.7717502117156982 at i=80\n",
            "Train 2.7310612201690674 at i=90\n",
            "Validation 2.636949062347412 at i=90\n",
            "Train 2.618427038192749 at i=100\n",
            "Validation 2.553110122680664 at i=100\n",
            "Train 2.53177547454834 at i=110\n",
            "Validation 2.4877684116363525 at i=110\n",
            "Train 2.4818172454833984 at i=120\n",
            "Validation 2.4407877922058105 at i=120\n",
            "Train 2.4499502182006836 at i=130\n",
            "Validation 2.4117431640625 at i=130\n",
            "Train 2.4195399284362793 at i=140\n",
            "Validation 2.407090425491333 at i=140\n",
            "Train 2.4091296195983887 at i=150\n",
            "Validation 2.384453773498535 at i=150\n",
            "Train 2.399857759475708 at i=160\n",
            "Validation 2.3579635620117188 at i=160\n",
            "Train 2.3763368129730225 at i=170\n",
            "Validation 2.3737151622772217 at i=170\n",
            "Train 2.3699474334716797 at i=180\n",
            "Validation 2.3629910945892334 at i=180\n",
            "Train 2.3639118671417236 at i=190\n",
            "Validation 2.3393430709838867 at i=190\n",
            "Train 2.355271100997925 at i=200\n",
            "Validation 2.3399479389190674 at i=200\n",
            "Train 2.348085880279541 at i=210\n",
            "Validation 2.326500177383423 at i=210\n",
            "Train 2.3383357524871826 at i=220\n",
            "Validation 2.325834274291992 at i=220\n",
            "Train 2.334442615509033 at i=230\n",
            "Validation 2.3294522762298584 at i=230\n",
            "Train 2.330606460571289 at i=240\n",
            "Validation 2.316500663757324 at i=240\n",
            "Train 2.3268368244171143 at i=250\n",
            "Validation 2.313342571258545 at i=250\n",
            "Train 2.3233706951141357 at i=260\n",
            "Validation 2.2905325889587402 at i=260\n",
            "Train 2.318389415740967 at i=270\n",
            "Validation 2.28771710395813 at i=270\n",
            "Train 2.3134818077087402 at i=280\n",
            "Validation 2.298177719116211 at i=280\n",
            "Train 2.304536819458008 at i=290\n",
            "Validation 2.27962589263916 at i=290\n",
            "Train 2.310756206512451 at i=300\n",
            "Validation 2.3019919395446777 at i=300\n",
            "Train 2.2999117374420166 at i=310\n",
            "Validation 2.29196834564209 at i=310\n",
            "Train 2.295515298843384 at i=320\n",
            "Validation 2.289236068725586 at i=320\n",
            "Train 2.2980031967163086 at i=330\n",
            "Validation 2.2734758853912354 at i=330\n",
            "Train 2.2869527339935303 at i=340\n",
            "Validation 2.274928569793701 at i=340\n",
            "Train 2.29264497756958 at i=350\n",
            "Validation 2.26336407661438 at i=350\n",
            "Train 2.2780728340148926 at i=360\n",
            "Validation 2.2754321098327637 at i=360\n",
            "Train 2.284515619277954 at i=370\n",
            "Validation 2.2848072052001953 at i=370\n",
            "Train 2.284275531768799 at i=380\n",
            "Validation 2.275597095489502 at i=380\n",
            "Train 2.280888557434082 at i=390\n",
            "Validation 2.2655251026153564 at i=390\n",
            "Train 2.2764792442321777 at i=400\n",
            "Validation 2.2601475715637207 at i=400\n",
            "Train 2.2708468437194824 at i=410\n",
            "Validation 2.279144048690796 at i=410\n",
            "Train 2.2761263847351074 at i=420\n",
            "Validation 2.2538347244262695 at i=420\n",
            "Train 2.270350694656372 at i=430\n",
            "Validation 2.244004487991333 at i=430\n",
            "Train 2.2619194984436035 at i=440\n",
            "Validation 2.276021957397461 at i=440\n",
            "Train 2.2563984394073486 at i=450\n",
            "Validation 2.2592968940734863 at i=450\n",
            "Train 2.2652595043182373 at i=460\n",
            "Validation 2.2471089363098145 at i=460\n",
            "Train 2.2545340061187744 at i=470\n",
            "Validation 2.2427854537963867 at i=470\n",
            "Train 2.2521252632141113 at i=480\n",
            "Validation 2.2431209087371826 at i=480\n",
            "Train 2.2510476112365723 at i=490\n",
            "Validation 2.2238569259643555 at i=490\n",
            "Train 2.253749370574951 at i=500\n",
            "Validation 2.2594902515411377 at i=500\n",
            "Train 2.2528388500213623 at i=510\n",
            "Validation 2.2328057289123535 at i=510\n",
            "Train 2.2479372024536133 at i=520\n",
            "Validation 2.228703737258911 at i=520\n",
            "Train 2.242142677307129 at i=530\n",
            "Validation 2.2281391620635986 at i=530\n",
            "Train 2.251192092895508 at i=540\n",
            "Validation 2.2368903160095215 at i=540\n",
            "Train 2.2465789318084717 at i=550\n",
            "Validation 2.2422337532043457 at i=550\n",
            "Train 2.238602876663208 at i=560\n",
            "Validation 2.24163556098938 at i=560\n",
            "Train 2.2445671558380127 at i=570\n",
            "Validation 2.2142014503479004 at i=570\n",
            "Train 2.227119207382202 at i=580\n",
            "Validation 2.197758436203003 at i=580\n",
            "Train 2.229828357696533 at i=590\n",
            "Validation 2.2120184898376465 at i=590\n",
            "Train 2.2315964698791504 at i=600\n",
            "Validation 2.2072482109069824 at i=600\n",
            "Train 2.2328929901123047 at i=610\n",
            "Validation 2.2001709938049316 at i=610\n",
            "Train 2.2256007194519043 at i=620\n",
            "Validation 2.2244668006896973 at i=620\n",
            "Train 2.228046178817749 at i=630\n",
            "Validation 2.214381694793701 at i=630\n",
            "Train 2.2186548709869385 at i=640\n",
            "Validation 2.196580410003662 at i=640\n",
            "Train 2.2155299186706543 at i=650\n",
            "Validation 2.2137928009033203 at i=650\n",
            "Train 2.220669746398926 at i=660\n",
            "Validation 2.2071902751922607 at i=660\n",
            "Train 2.214505434036255 at i=670\n",
            "Validation 2.1897835731506348 at i=670\n",
            "Train 2.203498363494873 at i=680\n",
            "Validation 2.2014245986938477 at i=680\n",
            "Train 2.215085029602051 at i=690\n",
            "Validation 2.2020912170410156 at i=690\n",
            "Train 2.2100460529327393 at i=700\n",
            "Validation 2.1882472038269043 at i=700\n",
            "Train 2.207880735397339 at i=710\n",
            "Validation 2.187011241912842 at i=710\n",
            "Train 2.2037668228149414 at i=720\n",
            "Validation 2.198335886001587 at i=720\n",
            "Train 2.2108592987060547 at i=730\n",
            "Validation 2.187800884246826 at i=730\n",
            "Train 2.200599431991577 at i=740\n",
            "Validation 2.1806859970092773 at i=740\n",
            "Train 2.1977157592773438 at i=750\n",
            "Validation 2.172135829925537 at i=750\n",
            "Train 2.1930651664733887 at i=760\n",
            "Validation 2.1660616397857666 at i=760\n",
            "Train 2.2005608081817627 at i=770\n",
            "Validation 2.2124829292297363 at i=770\n",
            "Train 2.1959967613220215 at i=780\n",
            "Validation 2.1750075817108154 at i=780\n",
            "Train 2.1929361820220947 at i=790\n",
            "Validation 2.179692268371582 at i=790\n",
            "Train 2.196229934692383 at i=800\n",
            "Validation 2.18595027923584 at i=800\n",
            "Train 2.1974587440490723 at i=810\n",
            "Validation 2.1885104179382324 at i=810\n",
            "Train 2.1884195804595947 at i=820\n",
            "Validation 2.1660962104797363 at i=820\n",
            "Train 2.1951184272766113 at i=830\n",
            "Validation 2.172691583633423 at i=830\n",
            "Train 2.195013999938965 at i=840\n",
            "Validation 2.2021255493164062 at i=840\n",
            "Train 2.1871163845062256 at i=850\n",
            "Validation 2.150446891784668 at i=850\n",
            "Train 2.1903398036956787 at i=860\n",
            "Validation 2.1610097885131836 at i=860\n",
            "Train 2.1842427253723145 at i=870\n",
            "Validation 2.161477565765381 at i=870\n",
            "Train 2.1875267028808594 at i=880\n",
            "Validation 2.1718859672546387 at i=880\n",
            "Train 2.183917999267578 at i=890\n",
            "Validation 2.1550827026367188 at i=890\n",
            "Train 2.1806817054748535 at i=900\n",
            "Validation 2.176147937774658 at i=900\n",
            "Train 2.1764862537384033 at i=910\n",
            "Validation 2.172182559967041 at i=910\n",
            "Train 2.1822762489318848 at i=920\n",
            "Validation 2.1515872478485107 at i=920\n",
            "Train 2.1775219440460205 at i=930\n",
            "Validation 2.168348789215088 at i=930\n",
            "Train 2.176973819732666 at i=940\n",
            "Validation 2.1584744453430176 at i=940\n",
            "Train 2.1676011085510254 at i=950\n",
            "Validation 2.152599334716797 at i=950\n",
            "Train 2.1715035438537598 at i=960\n",
            "Validation 2.148756504058838 at i=960\n",
            "Train 2.1678709983825684 at i=970\n",
            "Validation 2.1468749046325684 at i=970\n",
            "Train 2.167476177215576 at i=980\n",
            "Validation 2.1231675148010254 at i=980\n",
            "Train 2.169882297515869 at i=990\n",
            "Validation 2.152993679046631 at i=990\n",
            "Train 2.1650025844573975 at i=1000\n",
            "Validation 2.1654317378997803 at i=1000\n",
            "Train 2.168844699859619 at i=1010\n",
            "Validation 2.134955644607544 at i=1010\n",
            "Train 2.160581350326538 at i=1020\n",
            "Validation 2.145698070526123 at i=1020\n",
            "Train 2.160271167755127 at i=1030\n",
            "Validation 2.1271026134490967 at i=1030\n",
            "Train 2.161546468734741 at i=1040\n",
            "Validation 2.1416404247283936 at i=1040\n",
            "Train 2.1563820838928223 at i=1050\n",
            "Validation 2.1443586349487305 at i=1050\n",
            "Train 2.16206431388855 at i=1060\n",
            "Validation 2.1430461406707764 at i=1060\n",
            "Train 2.1633224487304688 at i=1070\n",
            "Validation 2.1423070430755615 at i=1070\n",
            "Train 2.1528635025024414 at i=1080\n",
            "Validation 2.14174485206604 at i=1080\n",
            "Train 2.163804769515991 at i=1090\n",
            "Validation 2.1407394409179688 at i=1090\n",
            "Train 2.1537230014801025 at i=1100\n",
            "Validation 2.1294445991516113 at i=1100\n",
            "Train 2.157379388809204 at i=1110\n",
            "Validation 2.1361989974975586 at i=1110\n",
            "Train 2.148247003555298 at i=1120\n",
            "Validation 2.1130306720733643 at i=1120\n",
            "Train 2.1483333110809326 at i=1130\n",
            "Validation 2.111668586730957 at i=1130\n",
            "Train 2.151562213897705 at i=1140\n",
            "Validation 2.112539768218994 at i=1140\n",
            "Train 2.1500067710876465 at i=1150\n",
            "Validation 2.1345181465148926 at i=1150\n",
            "Train 2.142955780029297 at i=1160\n",
            "Validation 2.1174709796905518 at i=1160\n",
            "Train 2.1420693397521973 at i=1170\n",
            "Validation 2.1130638122558594 at i=1170\n",
            "Train 2.1365556716918945 at i=1180\n",
            "Validation 2.1090526580810547 at i=1180\n",
            "Train 2.142383337020874 at i=1190\n",
            "Validation 2.1059207916259766 at i=1190\n",
            "Train 2.1341726779937744 at i=1200\n",
            "Validation 2.1214303970336914 at i=1200\n",
            "Train 2.1413493156433105 at i=1210\n",
            "Validation 2.092775344848633 at i=1210\n",
            "Train 2.1389870643615723 at i=1220\n",
            "Validation 2.094413995742798 at i=1220\n",
            "Train 2.1285858154296875 at i=1230\n",
            "Validation 2.0960044860839844 at i=1230\n",
            "Train 2.1279759407043457 at i=1240\n",
            "Validation 2.0874290466308594 at i=1240\n",
            "Train 2.1228291988372803 at i=1250\n",
            "Validation 2.0700793266296387 at i=1250\n",
            "Train 2.138124704360962 at i=1260\n",
            "Validation 2.079221725463867 at i=1260\n",
            "Train 2.1255059242248535 at i=1270\n",
            "Validation 2.0789170265197754 at i=1270\n",
            "Train 2.119049310684204 at i=1280\n",
            "Validation 2.069098472595215 at i=1280\n",
            "Train 2.1115665435791016 at i=1290\n",
            "Validation 2.066190719604492 at i=1290\n",
            "Train 2.1127352714538574 at i=1300\n",
            "Validation 2.087740898132324 at i=1300\n",
            "Train 2.1039459705352783 at i=1310\n",
            "Validation 2.038815498352051 at i=1310\n",
            "Train 2.1043081283569336 at i=1320\n",
            "Validation 2.0437889099121094 at i=1320\n",
            "Train 2.103355884552002 at i=1330\n",
            "Validation 2.044796943664551 at i=1330\n",
            "Train 2.0937304496765137 at i=1340\n",
            "Validation 2.0328781604766846 at i=1340\n",
            "Train 2.096501350402832 at i=1350\n",
            "Validation 2.0490031242370605 at i=1350\n",
            "Train 2.095778703689575 at i=1360\n",
            "Validation 2.0309596061706543 at i=1360\n",
            "Train 2.091942310333252 at i=1370\n",
            "Validation 2.046746015548706 at i=1370\n",
            "Train 2.081890821456909 at i=1380\n",
            "Validation 2.0308778285980225 at i=1380\n",
            "Train 2.0760700702667236 at i=1390\n",
            "Validation 2.0343048572540283 at i=1390\n",
            "Train 2.0820565223693848 at i=1400\n",
            "Validation 2.0363550186157227 at i=1400\n",
            "Train 2.0859320163726807 at i=1410\n",
            "Validation 2.0032145977020264 at i=1410\n",
            "Train 2.075334072113037 at i=1420\n",
            "Validation 1.9914841651916504 at i=1420\n",
            "Train 2.0721471309661865 at i=1430\n",
            "Validation 2.010711908340454 at i=1430\n",
            "Train 2.0774102210998535 at i=1440\n",
            "Validation 1.9859857559204102 at i=1440\n",
            "Train 2.0713119506835938 at i=1450\n",
            "Validation 1.9854638576507568 at i=1450\n",
            "Train 2.063514471054077 at i=1460\n",
            "Validation 1.9993839263916016 at i=1460\n",
            "Train 2.0586490631103516 at i=1470\n",
            "Validation 2.014146327972412 at i=1470\n",
            "Train 2.061391830444336 at i=1480\n",
            "Validation 1.9755257368087769 at i=1480\n",
            "Train 2.0511043071746826 at i=1490\n",
            "Validation 1.959012508392334 at i=1490\n",
            "Train 2.052762508392334 at i=1500\n",
            "Validation 1.9742457866668701 at i=1500\n",
            "Train 2.044412136077881 at i=1510\n",
            "Validation 1.9749748706817627 at i=1510\n",
            "Train 2.040846347808838 at i=1520\n",
            "Validation 1.958171010017395 at i=1520\n",
            "Train 2.041217565536499 at i=1530\n",
            "Validation 1.9860113859176636 at i=1530\n",
            "Train 2.0452237129211426 at i=1540\n",
            "Validation 1.9803011417388916 at i=1540\n",
            "Train 2.032604217529297 at i=1550\n",
            "Validation 1.961453914642334 at i=1550\n",
            "Train 2.031602621078491 at i=1560\n",
            "Validation 1.9586586952209473 at i=1560\n",
            "Train 2.029263973236084 at i=1570\n",
            "Validation 1.932783603668213 at i=1570\n",
            "Train 2.0150668621063232 at i=1580\n",
            "Validation 1.9361071586608887 at i=1580\n",
            "Train 2.020315647125244 at i=1590\n",
            "Validation 1.9145981073379517 at i=1590\n",
            "Train 2.0271074771881104 at i=1600\n",
            "Validation 1.9509330987930298 at i=1600\n",
            "Train 2.0039305686950684 at i=1610\n",
            "Validation 1.9296989440917969 at i=1610\n",
            "Train 2.004324436187744 at i=1620\n",
            "Validation 1.932525873184204 at i=1620\n",
            "Train 2.002471446990967 at i=1630\n",
            "Validation 1.9200206995010376 at i=1630\n",
            "Train 1.9992878437042236 at i=1640\n",
            "Validation 1.9145617485046387 at i=1640\n",
            "Train 2.0007665157318115 at i=1650\n",
            "Validation 1.9046697616577148 at i=1650\n",
            "Train 1.989876389503479 at i=1660\n",
            "Validation 1.9232796430587769 at i=1660\n",
            "Train 1.9831336736679077 at i=1670\n",
            "Validation 1.911081075668335 at i=1670\n",
            "Train 1.9831161499023438 at i=1680\n",
            "Validation 1.9047274589538574 at i=1680\n",
            "Train 1.9737157821655273 at i=1690\n",
            "Validation 1.8993861675262451 at i=1690\n",
            "Train 1.972098708152771 at i=1700\n",
            "Validation 1.9032015800476074 at i=1700\n",
            "Train 1.965134859085083 at i=1710\n",
            "Validation 1.8784894943237305 at i=1710\n",
            "Train 1.9658710956573486 at i=1720\n",
            "Validation 1.8608258962631226 at i=1720\n",
            "Train 1.9559710025787354 at i=1730\n",
            "Validation 1.8873910903930664 at i=1730\n",
            "Train 1.9444968700408936 at i=1740\n",
            "Validation 1.893686294555664 at i=1740\n",
            "Train 1.9495198726654053 at i=1750\n",
            "Validation 1.851844072341919 at i=1750\n",
            "Train 1.9498497247695923 at i=1760\n",
            "Validation 1.8604984283447266 at i=1760\n",
            "Train 1.9505717754364014 at i=1770\n",
            "Validation 1.8438524007797241 at i=1770\n",
            "Train 1.9392755031585693 at i=1780\n",
            "Validation 1.864877700805664 at i=1780\n",
            "Train 1.9398419857025146 at i=1790\n",
            "Validation 1.8203294277191162 at i=1790\n",
            "Train 1.931390404701233 at i=1800\n",
            "Validation 1.826573133468628 at i=1800\n",
            "Train 1.9172594547271729 at i=1810\n",
            "Validation 1.8331921100616455 at i=1810\n",
            "Train 1.918033242225647 at i=1820\n",
            "Validation 1.8125214576721191 at i=1820\n",
            "Train 1.9038711786270142 at i=1830\n",
            "Validation 1.7971186637878418 at i=1830\n",
            "Train 1.9097455739974976 at i=1840\n",
            "Validation 1.7769709825515747 at i=1840\n",
            "Train 1.9067485332489014 at i=1850\n",
            "Validation 1.7847716808319092 at i=1850\n",
            "Train 1.8943684101104736 at i=1860\n",
            "Validation 1.7751795053482056 at i=1860\n",
            "Train 1.8976192474365234 at i=1870\n",
            "Validation 1.8017168045043945 at i=1870\n",
            "Train 1.8916397094726562 at i=1880\n",
            "Validation 1.7667176723480225 at i=1880\n",
            "Train 1.889548659324646 at i=1890\n",
            "Validation 1.7728605270385742 at i=1890\n",
            "Train 1.8780229091644287 at i=1900\n",
            "Validation 1.803055763244629 at i=1900\n",
            "Train 1.8814386129379272 at i=1910\n",
            "Validation 1.7599581480026245 at i=1910\n",
            "Train 1.870446801185608 at i=1920\n",
            "Validation 1.7431293725967407 at i=1920\n",
            "Train 1.8675472736358643 at i=1930\n",
            "Validation 1.774830937385559 at i=1930\n",
            "Train 1.869362473487854 at i=1940\n",
            "Validation 1.7437024116516113 at i=1940\n",
            "Train 1.8655126094818115 at i=1950\n",
            "Validation 1.7458488941192627 at i=1950\n",
            "Train 1.8647005558013916 at i=1960\n",
            "Validation 1.745108962059021 at i=1960\n",
            "Train 1.8545255661010742 at i=1970\n",
            "Validation 1.7411558628082275 at i=1970\n",
            "Train 1.8431074619293213 at i=1980\n",
            "Validation 1.743189811706543 at i=1980\n",
            "Train 1.8384666442871094 at i=1990\n",
            "Validation 1.7038509845733643 at i=1990\n",
            "Train 1.8368291854858398 at i=2000\n",
            "Validation 1.7401893138885498 at i=2000\n",
            "Train 1.8354737758636475 at i=2010\n",
            "Validation 1.7221617698669434 at i=2010\n",
            "Train 1.8303569555282593 at i=2020\n",
            "Validation 1.7383307218551636 at i=2020\n",
            "Train 1.8252023458480835 at i=2030\n",
            "Validation 1.760326862335205 at i=2030\n",
            "Train 1.832350492477417 at i=2040\n",
            "Validation 1.7124619483947754 at i=2040\n",
            "Train 1.8174495697021484 at i=2050\n",
            "Validation 1.7060089111328125 at i=2050\n",
            "Train 1.820364236831665 at i=2060\n",
            "Validation 1.7126836776733398 at i=2060\n",
            "Train 1.822149634361267 at i=2070\n",
            "Validation 1.68329656124115 at i=2070\n",
            "Train 1.803754448890686 at i=2080\n",
            "Validation 1.7071058750152588 at i=2080\n",
            "Train 1.8099206686019897 at i=2090\n",
            "Validation 1.6824593544006348 at i=2090\n",
            "Train 1.808095932006836 at i=2100\n",
            "Validation 1.6765682697296143 at i=2100\n",
            "Train 1.79547119140625 at i=2110\n",
            "Validation 1.7140002250671387 at i=2110\n",
            "Train 1.7993383407592773 at i=2120\n",
            "Validation 1.683591365814209 at i=2120\n",
            "Train 1.7951446771621704 at i=2130\n",
            "Validation 1.647168755531311 at i=2130\n",
            "Train 1.7963435649871826 at i=2140\n",
            "Validation 1.675565481185913 at i=2140\n",
            "Train 1.7870855331420898 at i=2150\n",
            "Validation 1.6847214698791504 at i=2150\n",
            "Train 1.7876123189926147 at i=2160\n",
            "Validation 1.6572425365447998 at i=2160\n",
            "Train 1.7708263397216797 at i=2170\n",
            "Validation 1.6519341468811035 at i=2170\n",
            "Train 1.7838093042373657 at i=2180\n",
            "Validation 1.6349976062774658 at i=2180\n",
            "Train 1.7767709493637085 at i=2190\n",
            "Validation 1.6291316747665405 at i=2190\n",
            "Train 1.7724415063858032 at i=2200\n",
            "Validation 1.6680123805999756 at i=2200\n",
            "Train 1.7744665145874023 at i=2210\n",
            "Validation 1.6662373542785645 at i=2210\n",
            "Train 1.7759478092193604 at i=2220\n",
            "Validation 1.6568808555603027 at i=2220\n",
            "Train 1.7630239725112915 at i=2230\n",
            "Validation 1.613257884979248 at i=2230\n",
            "Train 1.759608507156372 at i=2240\n",
            "Validation 1.6522164344787598 at i=2240\n",
            "Train 1.7474855184555054 at i=2250\n",
            "Validation 1.6147410869598389 at i=2250\n",
            "Train 1.748957633972168 at i=2260\n",
            "Validation 1.6506462097167969 at i=2260\n",
            "Train 1.7382854223251343 at i=2270\n",
            "Validation 1.592790126800537 at i=2270\n",
            "Train 1.7545582056045532 at i=2280\n",
            "Validation 1.6334190368652344 at i=2280\n",
            "Train 1.7401459217071533 at i=2290\n",
            "Validation 1.627140998840332 at i=2290\n",
            "Train 1.7405948638916016 at i=2300\n",
            "Validation 1.639707088470459 at i=2300\n",
            "Train 1.7321455478668213 at i=2310\n",
            "Validation 1.58015775680542 at i=2310\n",
            "Train 1.7284162044525146 at i=2320\n",
            "Validation 1.6335914134979248 at i=2320\n",
            "Train 1.729917287826538 at i=2330\n",
            "Validation 1.5926918983459473 at i=2330\n",
            "Train 1.7073309421539307 at i=2340\n",
            "Validation 1.5756088495254517 at i=2340\n",
            "Train 1.7193653583526611 at i=2350\n",
            "Validation 1.5943050384521484 at i=2350\n",
            "Train 1.7183563709259033 at i=2360\n",
            "Validation 1.5894073247909546 at i=2360\n",
            "Train 1.7191683053970337 at i=2370\n",
            "Validation 1.5711305141448975 at i=2370\n",
            "Train 1.7064138650894165 at i=2380\n",
            "Validation 1.5603727102279663 at i=2380\n",
            "Train 1.7068580389022827 at i=2390\n",
            "Validation 1.5584062337875366 at i=2390\n",
            "Train 1.7123241424560547 at i=2400\n",
            "Validation 1.5937386751174927 at i=2400\n",
            "Train 1.7020998001098633 at i=2410\n",
            "Validation 1.586402177810669 at i=2410\n",
            "Train 1.6972852945327759 at i=2420\n",
            "Validation 1.573710322380066 at i=2420\n",
            "Train 1.6970164775848389 at i=2430\n",
            "Validation 1.5878773927688599 at i=2430\n",
            "Train 1.6863771677017212 at i=2440\n",
            "Validation 1.5553956031799316 at i=2440\n",
            "Train 1.6881024837493896 at i=2450\n",
            "Validation 1.5455899238586426 at i=2450\n",
            "Train 1.6841280460357666 at i=2460\n",
            "Validation 1.5615663528442383 at i=2460\n",
            "Train 1.6736301183700562 at i=2470\n",
            "Validation 1.5252536535263062 at i=2470\n",
            "Train 1.6863515377044678 at i=2480\n",
            "Validation 1.5386135578155518 at i=2480\n",
            "Train 1.6744089126586914 at i=2490\n",
            "Validation 1.541918158531189 at i=2490\n",
            "Train 1.6695702075958252 at i=2500\n",
            "Validation 1.5713121891021729 at i=2500\n",
            "Train 1.6823017597198486 at i=2510\n",
            "Validation 1.5526270866394043 at i=2510\n",
            "Train 1.6769520044326782 at i=2520\n",
            "Validation 1.5471150875091553 at i=2520\n",
            "Train 1.6597810983657837 at i=2530\n",
            "Validation 1.562650442123413 at i=2530\n",
            "Train 1.679579734802246 at i=2540\n",
            "Validation 1.563797950744629 at i=2540\n",
            "Train 1.666948676109314 at i=2550\n",
            "Validation 1.515789270401001 at i=2550\n",
            "Train 1.6609350442886353 at i=2560\n",
            "Validation 1.5234384536743164 at i=2560\n",
            "Train 1.6515388488769531 at i=2570\n",
            "Validation 1.534271001815796 at i=2570\n",
            "Train 1.6569669246673584 at i=2580\n",
            "Validation 1.5236859321594238 at i=2580\n",
            "Train 1.6632506847381592 at i=2590\n",
            "Validation 1.5197069644927979 at i=2590\n",
            "Train 1.6592702865600586 at i=2600\n",
            "Validation 1.5432815551757812 at i=2600\n",
            "Train 1.6435816287994385 at i=2610\n",
            "Validation 1.5237762928009033 at i=2610\n",
            "Train 1.658776879310608 at i=2620\n",
            "Validation 1.4992139339447021 at i=2620\n",
            "Train 1.630190134048462 at i=2630\n",
            "Validation 1.5147578716278076 at i=2630\n",
            "Train 1.637381911277771 at i=2640\n",
            "Validation 1.4995112419128418 at i=2640\n",
            "Train 1.6416828632354736 at i=2650\n",
            "Validation 1.5013718605041504 at i=2650\n",
            "Train 1.6432358026504517 at i=2660\n",
            "Validation 1.4769620895385742 at i=2660\n",
            "Train 1.6335843801498413 at i=2670\n",
            "Validation 1.4984776973724365 at i=2670\n",
            "Train 1.6341798305511475 at i=2680\n",
            "Validation 1.5204875469207764 at i=2680\n",
            "Train 1.6323267221450806 at i=2690\n",
            "Validation 1.5050034523010254 at i=2690\n",
            "Train 1.6375055313110352 at i=2700\n",
            "Validation 1.5187610387802124 at i=2700\n",
            "Train 1.6252944469451904 at i=2710\n",
            "Validation 1.5025568008422852 at i=2710\n",
            "Train 1.6142308712005615 at i=2720\n",
            "Validation 1.5006203651428223 at i=2720\n",
            "Train 1.6157782077789307 at i=2730\n",
            "Validation 1.479928731918335 at i=2730\n",
            "Train 1.617832899093628 at i=2740\n",
            "Validation 1.5172219276428223 at i=2740\n",
            "Train 1.6269874572753906 at i=2750\n",
            "Validation 1.5215624570846558 at i=2750\n",
            "Train 1.608755111694336 at i=2760\n",
            "Validation 1.5091638565063477 at i=2760\n",
            "Train 1.6102135181427002 at i=2770\n",
            "Validation 1.4884058237075806 at i=2770\n",
            "Train 1.6103090047836304 at i=2780\n",
            "Validation 1.4790165424346924 at i=2780\n",
            "Train 1.6131360530853271 at i=2790\n",
            "Validation 1.5077718496322632 at i=2790\n",
            "Train 1.6151952743530273 at i=2800\n",
            "Validation 1.4935520887374878 at i=2800\n",
            "Train 1.6122734546661377 at i=2810\n",
            "Validation 1.4578609466552734 at i=2810\n",
            "Train 1.6076021194458008 at i=2820\n",
            "Validation 1.465919017791748 at i=2820\n",
            "Train 1.5899733304977417 at i=2830\n",
            "Validation 1.4321098327636719 at i=2830\n",
            "Train 1.5941284894943237 at i=2840\n",
            "Validation 1.4413665533065796 at i=2840\n",
            "Train 1.607038140296936 at i=2850\n",
            "Validation 1.4383864402770996 at i=2850\n",
            "Train 1.5985016822814941 at i=2860\n",
            "Validation 1.4351372718811035 at i=2860\n",
            "Train 1.5973464250564575 at i=2870\n",
            "Validation 1.447034478187561 at i=2870\n",
            "Train 1.5931501388549805 at i=2880\n",
            "Validation 1.4641425609588623 at i=2880\n",
            "Train 1.5901899337768555 at i=2890\n",
            "Validation 1.4321112632751465 at i=2890\n",
            "Train 1.5896341800689697 at i=2900\n",
            "Validation 1.4351401329040527 at i=2900\n",
            "Train 1.58968985080719 at i=2910\n",
            "Validation 1.445218801498413 at i=2910\n",
            "Train 1.5922424793243408 at i=2920\n",
            "Validation 1.464473009109497 at i=2920\n",
            "Train 1.5941540002822876 at i=2930\n",
            "Validation 1.4352015256881714 at i=2930\n",
            "Train 1.581555724143982 at i=2940\n",
            "Validation 1.4562439918518066 at i=2940\n",
            "Train 1.578142762184143 at i=2950\n",
            "Validation 1.4590555429458618 at i=2950\n",
            "Train 1.5887666940689087 at i=2960\n",
            "Validation 1.4219369888305664 at i=2960\n",
            "Train 1.5713584423065186 at i=2970\n",
            "Validation 1.4700798988342285 at i=2970\n",
            "Train 1.5700517892837524 at i=2980\n",
            "Validation 1.452186107635498 at i=2980\n",
            "Train 1.5706229209899902 at i=2990\n",
            "Validation 1.4339755773544312 at i=2990\n",
            "Train 1.5740249156951904 at i=3000\n",
            "Validation 1.4515084028244019 at i=3000\n",
            "Train 1.552001714706421 at i=3010\n",
            "Validation 1.4116215705871582 at i=3010\n",
            "Train 1.5647774934768677 at i=3020\n",
            "Validation 1.421069860458374 at i=3020\n",
            "Train 1.565545916557312 at i=3030\n",
            "Validation 1.4145896434783936 at i=3030\n",
            "Train 1.5472943782806396 at i=3040\n",
            "Validation 1.4233131408691406 at i=3040\n",
            "Train 1.5634520053863525 at i=3050\n",
            "Validation 1.4149163961410522 at i=3050\n",
            "Train 1.5402013063430786 at i=3060\n",
            "Validation 1.4574471712112427 at i=3060\n",
            "Train 1.546015977859497 at i=3070\n",
            "Validation 1.4297089576721191 at i=3070\n",
            "Train 1.5560916662216187 at i=3080\n",
            "Validation 1.4119393825531006 at i=3080\n",
            "Train 1.5581949949264526 at i=3090\n",
            "Validation 1.4742978811264038 at i=3090\n",
            "Train 1.5542460680007935 at i=3100\n",
            "Validation 1.3950437307357788 at i=3100\n",
            "Train 1.5607692003250122 at i=3110\n",
            "Validation 1.4280248880386353 at i=3110\n",
            "Train 1.5462661981582642 at i=3120\n",
            "Validation 1.4207345247268677 at i=3120\n",
            "Train 1.5521776676177979 at i=3130\n",
            "Validation 1.3860485553741455 at i=3130\n",
            "Train 1.5575520992279053 at i=3140\n",
            "Validation 1.4272091388702393 at i=3140\n",
            "Train 1.5454168319702148 at i=3150\n",
            "Validation 1.4032198190689087 at i=3150\n",
            "Train 1.5366578102111816 at i=3160\n",
            "Validation 1.40543532371521 at i=3160\n",
            "Train 1.5409457683563232 at i=3170\n",
            "Validation 1.3918185234069824 at i=3170\n",
            "Train 1.5457477569580078 at i=3180\n",
            "Validation 1.4136695861816406 at i=3180\n",
            "Train 1.5322948694229126 at i=3190\n",
            "Validation 1.4375957250595093 at i=3190\n",
            "Train 1.529175043106079 at i=3200\n",
            "Validation 1.3897614479064941 at i=3200\n",
            "Train 1.530752182006836 at i=3210\n",
            "Validation 1.3932493925094604 at i=3210\n",
            "Train 1.530928373336792 at i=3220\n",
            "Validation 1.3824543952941895 at i=3220\n",
            "Train 1.5277812480926514 at i=3230\n",
            "Validation 1.412571907043457 at i=3230\n",
            "Train 1.5323474407196045 at i=3240\n",
            "Validation 1.3681917190551758 at i=3240\n",
            "Train 1.536583662033081 at i=3250\n",
            "Validation 1.4013538360595703 at i=3250\n",
            "Train 1.5264708995819092 at i=3260\n",
            "Validation 1.376039743423462 at i=3260\n",
            "Train 1.5332814455032349 at i=3270\n",
            "Validation 1.4151310920715332 at i=3270\n",
            "Train 1.5222928524017334 at i=3280\n",
            "Validation 1.3536349534988403 at i=3280\n",
            "Train 1.5371544361114502 at i=3290\n",
            "Validation 1.4034101963043213 at i=3290\n",
            "Train 1.516765832901001 at i=3300\n",
            "Validation 1.3668062686920166 at i=3300\n",
            "Train 1.5241296291351318 at i=3310\n",
            "Validation 1.390589952468872 at i=3310\n",
            "Train 1.5225251913070679 at i=3320\n",
            "Validation 1.3682509660720825 at i=3320\n",
            "Train 1.51913321018219 at i=3330\n",
            "Validation 1.378051519393921 at i=3330\n",
            "Train 1.5161091089248657 at i=3340\n",
            "Validation 1.3783187866210938 at i=3340\n",
            "Train 1.5094447135925293 at i=3350\n",
            "Validation 1.3797413110733032 at i=3350\n",
            "Train 1.52262282371521 at i=3360\n",
            "Validation 1.3804126977920532 at i=3360\n",
            "Train 1.5291001796722412 at i=3370\n",
            "Validation 1.3785264492034912 at i=3370\n",
            "Train 1.5157616138458252 at i=3380\n",
            "Validation 1.3774924278259277 at i=3380\n",
            "Train 1.5036299228668213 at i=3390\n",
            "Validation 1.3841948509216309 at i=3390\n",
            "Train 1.513318657875061 at i=3400\n",
            "Validation 1.401041030883789 at i=3400\n",
            "Train 1.498259425163269 at i=3410\n",
            "Validation 1.3772175312042236 at i=3410\n",
            "Train 1.5199795961380005 at i=3420\n",
            "Validation 1.4072951078414917 at i=3420\n",
            "Train 1.507683277130127 at i=3430\n",
            "Validation 1.3755035400390625 at i=3430\n",
            "Train 1.5021523237228394 at i=3440\n",
            "Validation 1.3841211795806885 at i=3440\n",
            "Train 1.5027406215667725 at i=3450\n",
            "Validation 1.3570291996002197 at i=3450\n",
            "Train 1.497859239578247 at i=3460\n",
            "Validation 1.3767441511154175 at i=3460\n",
            "Train 1.4990214109420776 at i=3470\n",
            "Validation 1.3713011741638184 at i=3470\n",
            "Train 1.4970005750656128 at i=3480\n",
            "Validation 1.3898839950561523 at i=3480\n",
            "Train 1.5069767236709595 at i=3490\n",
            "Validation 1.3787565231323242 at i=3490\n",
            "Train 1.498476266860962 at i=3500\n",
            "Validation 1.38352632522583 at i=3500\n",
            "Train 1.4978721141815186 at i=3510\n",
            "Validation 1.373317003250122 at i=3510\n",
            "Train 1.4881541728973389 at i=3520\n",
            "Validation 1.3503789901733398 at i=3520\n",
            "Train 1.4818493127822876 at i=3530\n",
            "Validation 1.3497393131256104 at i=3530\n",
            "Train 1.4900163412094116 at i=3540\n",
            "Validation 1.3825498819351196 at i=3540\n",
            "Train 1.49092698097229 at i=3550\n",
            "Validation 1.3788343667984009 at i=3550\n",
            "Train 1.4861552715301514 at i=3560\n",
            "Validation 1.3942680358886719 at i=3560\n",
            "Train 1.486472487449646 at i=3570\n",
            "Validation 1.3532178401947021 at i=3570\n",
            "Train 1.4968278408050537 at i=3580\n",
            "Validation 1.3310707807540894 at i=3580\n",
            "Train 1.4803036451339722 at i=3590\n",
            "Validation 1.3524954319000244 at i=3590\n",
            "Train 1.4659430980682373 at i=3600\n",
            "Validation 1.35154128074646 at i=3600\n",
            "Train 1.4748961925506592 at i=3610\n",
            "Validation 1.3956925868988037 at i=3610\n",
            "Train 1.474130630493164 at i=3620\n",
            "Validation 1.3615121841430664 at i=3620\n",
            "Train 1.4834403991699219 at i=3630\n",
            "Validation 1.3700454235076904 at i=3630\n",
            "Train 1.4685813188552856 at i=3640\n",
            "Validation 1.383012294769287 at i=3640\n",
            "Train 1.474345088005066 at i=3650\n",
            "Validation 1.2931385040283203 at i=3650\n",
            "Train 1.457960844039917 at i=3660\n",
            "Validation 1.3632376194000244 at i=3660\n",
            "Train 1.475567102432251 at i=3670\n",
            "Validation 1.3310788869857788 at i=3670\n",
            "Train 1.479733943939209 at i=3680\n",
            "Validation 1.3473495244979858 at i=3680\n",
            "Train 1.463364601135254 at i=3690\n",
            "Validation 1.3138642311096191 at i=3690\n",
            "Train 1.4744256734848022 at i=3700\n",
            "Validation 1.3608272075653076 at i=3700\n",
            "Train 1.4707698822021484 at i=3710\n",
            "Validation 1.3386411666870117 at i=3710\n",
            "Train 1.4591363668441772 at i=3720\n",
            "Validation 1.3594512939453125 at i=3720\n",
            "Train 1.4680684804916382 at i=3730\n",
            "Validation 1.3287700414657593 at i=3730\n",
            "Train 1.4736398458480835 at i=3740\n",
            "Validation 1.3259599208831787 at i=3740\n",
            "Train 1.4658459424972534 at i=3750\n",
            "Validation 1.3338780403137207 at i=3750\n",
            "Train 1.4587453603744507 at i=3760\n",
            "Validation 1.3405885696411133 at i=3760\n",
            "Train 1.4605741500854492 at i=3770\n",
            "Validation 1.3391402959823608 at i=3770\n",
            "Train 1.4595813751220703 at i=3780\n",
            "Validation 1.3109116554260254 at i=3780\n",
            "Train 1.4585562944412231 at i=3790\n",
            "Validation 1.3861185312271118 at i=3790\n",
            "Train 1.4746228456497192 at i=3800\n",
            "Validation 1.344361424446106 at i=3800\n",
            "Train 1.4665582180023193 at i=3810\n",
            "Validation 1.2973628044128418 at i=3810\n",
            "Train 1.4665567874908447 at i=3820\n",
            "Validation 1.3394956588745117 at i=3820\n",
            "Train 1.4571893215179443 at i=3830\n",
            "Validation 1.3469427824020386 at i=3830\n",
            "Train 1.4612696170806885 at i=3840\n",
            "Validation 1.3204121589660645 at i=3840\n",
            "Train 1.4532525539398193 at i=3850\n",
            "Validation 1.319914698600769 at i=3850\n",
            "Train 1.452308177947998 at i=3860\n",
            "Validation 1.3336963653564453 at i=3860\n",
            "Train 1.467505931854248 at i=3870\n",
            "Validation 1.3361406326293945 at i=3870\n",
            "Train 1.4558740854263306 at i=3880\n",
            "Validation 1.3218743801116943 at i=3880\n",
            "Train 1.451141119003296 at i=3890\n",
            "Validation 1.3727936744689941 at i=3890\n",
            "Train 1.4636237621307373 at i=3900\n",
            "Validation 1.305898666381836 at i=3900\n",
            "Train 1.4544768333435059 at i=3910\n",
            "Validation 1.333816647529602 at i=3910\n",
            "Train 1.438144326210022 at i=3920\n",
            "Validation 1.3651865720748901 at i=3920\n",
            "Train 1.4429266452789307 at i=3930\n",
            "Validation 1.335503339767456 at i=3930\n",
            "Train 1.4487454891204834 at i=3940\n",
            "Validation 1.3019158840179443 at i=3940\n",
            "Train 1.4530439376831055 at i=3950\n",
            "Validation 1.3335444927215576 at i=3950\n",
            "Train 1.454497218132019 at i=3960\n",
            "Validation 1.3232009410858154 at i=3960\n",
            "Train 1.4421606063842773 at i=3970\n",
            "Validation 1.311970591545105 at i=3970\n",
            "Train 1.450261116027832 at i=3980\n",
            "Validation 1.3159875869750977 at i=3980\n",
            "Train 1.4513251781463623 at i=3990\n",
            "Validation 1.3384509086608887 at i=3990\n",
            "Train 1.4390455484390259 at i=4000\n",
            "Validation 1.338195562362671 at i=4000\n",
            "Train 1.441056251525879 at i=4010\n",
            "Validation 1.317448616027832 at i=4010\n",
            "Train 1.4446347951889038 at i=4020\n",
            "Validation 1.3137989044189453 at i=4020\n",
            "Train 1.436927080154419 at i=4030\n",
            "Validation 1.3394098281860352 at i=4030\n",
            "Train 1.4503713846206665 at i=4040\n",
            "Validation 1.3139179944992065 at i=4040\n",
            "Train 1.4372755289077759 at i=4050\n",
            "Validation 1.3378477096557617 at i=4050\n",
            "Train 1.432071328163147 at i=4060\n",
            "Validation 1.338821530342102 at i=4060\n",
            "Train 1.4384921789169312 at i=4070\n",
            "Validation 1.2678284645080566 at i=4070\n",
            "Train 1.4374531507492065 at i=4080\n",
            "Validation 1.323392629623413 at i=4080\n",
            "Train 1.429935097694397 at i=4090\n",
            "Validation 1.313889503479004 at i=4090\n",
            "Train 1.425872802734375 at i=4100\n",
            "Validation 1.300896167755127 at i=4100\n",
            "Train 1.4328430891036987 at i=4110\n",
            "Validation 1.3107271194458008 at i=4110\n",
            "Train 1.4230616092681885 at i=4120\n",
            "Validation 1.3147790431976318 at i=4120\n",
            "Train 1.4335434436798096 at i=4130\n",
            "Validation 1.3043560981750488 at i=4130\n",
            "Train 1.4209651947021484 at i=4140\n",
            "Validation 1.3075101375579834 at i=4140\n",
            "Train 1.434726357460022 at i=4150\n",
            "Validation 1.2897593975067139 at i=4150\n",
            "Train 1.419692039489746 at i=4160\n",
            "Validation 1.335740089416504 at i=4160\n",
            "Train 1.4206875562667847 at i=4170\n",
            "Validation 1.2912099361419678 at i=4170\n",
            "Train 1.426324486732483 at i=4180\n",
            "Validation 1.2991907596588135 at i=4180\n",
            "Train 1.4169175624847412 at i=4190\n",
            "Validation 1.3246039152145386 at i=4190\n",
            "Train 1.421750783920288 at i=4200\n",
            "Validation 1.3227717876434326 at i=4200\n",
            "Train 1.4185998439788818 at i=4210\n",
            "Validation 1.290806531906128 at i=4210\n",
            "Train 1.4330360889434814 at i=4220\n",
            "Validation 1.285597324371338 at i=4220\n",
            "Train 1.4143396615982056 at i=4230\n",
            "Validation 1.2843942642211914 at i=4230\n",
            "Train 1.4220468997955322 at i=4240\n",
            "Validation 1.2427505254745483 at i=4240\n",
            "Train 1.4224188327789307 at i=4250\n",
            "Validation 1.3280948400497437 at i=4250\n",
            "Train 1.4182970523834229 at i=4260\n",
            "Validation 1.2987165451049805 at i=4260\n",
            "Train 1.4097546339035034 at i=4270\n",
            "Validation 1.2958934307098389 at i=4270\n",
            "Train 1.40950608253479 at i=4280\n",
            "Validation 1.3292899131774902 at i=4280\n",
            "Train 1.4235966205596924 at i=4290\n",
            "Validation 1.3266562223434448 at i=4290\n",
            "Train 1.4149210453033447 at i=4300\n",
            "Validation 1.3093950748443604 at i=4300\n",
            "Train 1.4117196798324585 at i=4310\n",
            "Validation 1.3128321170806885 at i=4310\n",
            "Train 1.4191778898239136 at i=4320\n",
            "Validation 1.2546201944351196 at i=4320\n",
            "Train 1.4062658548355103 at i=4330\n",
            "Validation 1.2736380100250244 at i=4330\n",
            "Train 1.4130371809005737 at i=4340\n",
            "Validation 1.2989139556884766 at i=4340\n",
            "Train 1.4137966632843018 at i=4350\n",
            "Validation 1.2413089275360107 at i=4350\n",
            "Train 1.4206777811050415 at i=4360\n",
            "Validation 1.2829294204711914 at i=4360\n",
            "Train 1.4237277507781982 at i=4370\n",
            "Validation 1.2702717781066895 at i=4370\n",
            "Train 1.406304955482483 at i=4380\n",
            "Validation 1.282965064048767 at i=4380\n",
            "Train 1.3974616527557373 at i=4390\n",
            "Validation 1.2558077573776245 at i=4390\n",
            "Train 1.4072574377059937 at i=4400\n",
            "Validation 1.2879557609558105 at i=4400\n",
            "Train 1.3919532299041748 at i=4410\n",
            "Validation 1.2594879865646362 at i=4410\n",
            "Train 1.4117941856384277 at i=4420\n",
            "Validation 1.263043999671936 at i=4420\n",
            "Train 1.4127782583236694 at i=4430\n",
            "Validation 1.267456293106079 at i=4430\n",
            "Train 1.404653549194336 at i=4440\n",
            "Validation 1.3068077564239502 at i=4440\n",
            "Train 1.4053442478179932 at i=4450\n",
            "Validation 1.2707304954528809 at i=4450\n",
            "Train 1.4010374546051025 at i=4460\n",
            "Validation 1.309800148010254 at i=4460\n",
            "Train 1.410184383392334 at i=4470\n",
            "Validation 1.263059377670288 at i=4470\n",
            "Train 1.4070926904678345 at i=4480\n",
            "Validation 1.2569940090179443 at i=4480\n",
            "Train 1.402417540550232 at i=4490\n",
            "Validation 1.2830387353897095 at i=4490\n",
            "Train 1.3929277658462524 at i=4500\n",
            "Validation 1.3182640075683594 at i=4500\n",
            "Train 1.390570878982544 at i=4510\n",
            "Validation 1.2900550365447998 at i=4510\n",
            "Train 1.4076454639434814 at i=4520\n",
            "Validation 1.2955312728881836 at i=4520\n",
            "Train 1.398697853088379 at i=4530\n",
            "Validation 1.2900853157043457 at i=4530\n",
            "Train 1.3954455852508545 at i=4540\n",
            "Validation 1.2720896005630493 at i=4540\n",
            "Train 1.3980896472930908 at i=4550\n",
            "Validation 1.3086521625518799 at i=4550\n",
            "Train 1.4081566333770752 at i=4560\n",
            "Validation 1.2403440475463867 at i=4560\n",
            "Train 1.3990205526351929 at i=4570\n",
            "Validation 1.3111368417739868 at i=4570\n",
            "Train 1.3962875604629517 at i=4580\n",
            "Validation 1.2896814346313477 at i=4580\n",
            "Train 1.3873755931854248 at i=4590\n",
            "Validation 1.2949707508087158 at i=4590\n",
            "Train 1.4015798568725586 at i=4600\n",
            "Validation 1.2830884456634521 at i=4600\n",
            "Train 1.3934296369552612 at i=4610\n",
            "Validation 1.2962970733642578 at i=4610\n",
            "Train 1.3853404521942139 at i=4620\n",
            "Validation 1.2510557174682617 at i=4620\n",
            "Train 1.3950039148330688 at i=4630\n",
            "Validation 1.2762261629104614 at i=4630\n",
            "Train 1.3859153985977173 at i=4640\n",
            "Validation 1.2840030193328857 at i=4640\n",
            "Train 1.3964284658432007 at i=4650\n",
            "Validation 1.265606164932251 at i=4650\n",
            "Train 1.3896043300628662 at i=4660\n",
            "Validation 1.2766276597976685 at i=4660\n",
            "Train 1.385585069656372 at i=4670\n",
            "Validation 1.2869044542312622 at i=4670\n",
            "Train 1.3872559070587158 at i=4680\n",
            "Validation 1.2643320560455322 at i=4680\n",
            "Train 1.3912845849990845 at i=4690\n",
            "Validation 1.2561577558517456 at i=4690\n",
            "Train 1.3878732919692993 at i=4700\n",
            "Validation 1.2720258235931396 at i=4700\n",
            "Train 1.3760359287261963 at i=4710\n",
            "Validation 1.287150502204895 at i=4710\n",
            "Train 1.3816300630569458 at i=4720\n",
            "Validation 1.238935947418213 at i=4720\n",
            "Train 1.3871262073516846 at i=4730\n",
            "Validation 1.2674150466918945 at i=4730\n",
            "Train 1.3696072101593018 at i=4740\n",
            "Validation 1.2570042610168457 at i=4740\n",
            "Train 1.3758447170257568 at i=4750\n",
            "Validation 1.2532942295074463 at i=4750\n",
            "Train 1.3942443132400513 at i=4760\n",
            "Validation 1.2394294738769531 at i=4760\n",
            "Train 1.3800565004348755 at i=4770\n",
            "Validation 1.2827342748641968 at i=4770\n",
            "Train 1.3831735849380493 at i=4780\n",
            "Validation 1.2497873306274414 at i=4780\n",
            "Train 1.3850477933883667 at i=4790\n",
            "Validation 1.2263644933700562 at i=4790\n",
            "Train 1.3814351558685303 at i=4800\n",
            "Validation 1.2696194648742676 at i=4800\n",
            "Train 1.3845431804656982 at i=4810\n",
            "Validation 1.2754034996032715 at i=4810\n",
            "Train 1.383998990058899 at i=4820\n",
            "Validation 1.2756338119506836 at i=4820\n",
            "Train 1.369489312171936 at i=4830\n",
            "Validation 1.2359527349472046 at i=4830\n",
            "Train 1.3802952766418457 at i=4840\n",
            "Validation 1.2769997119903564 at i=4840\n",
            "Train 1.376410961151123 at i=4850\n",
            "Validation 1.2447229623794556 at i=4850\n",
            "Train 1.3756554126739502 at i=4860\n",
            "Validation 1.2617511749267578 at i=4860\n",
            "Train 1.3719745874404907 at i=4870\n",
            "Validation 1.242491364479065 at i=4870\n",
            "Train 1.3830749988555908 at i=4880\n",
            "Validation 1.2486340999603271 at i=4880\n",
            "Train 1.3784054517745972 at i=4890\n",
            "Validation 1.245370864868164 at i=4890\n",
            "Train 1.3828215599060059 at i=4900\n",
            "Validation 1.2393746376037598 at i=4900\n",
            "Train 1.375523328781128 at i=4910\n",
            "Validation 1.3308794498443604 at i=4910\n",
            "Train 1.3713653087615967 at i=4920\n",
            "Validation 1.2657215595245361 at i=4920\n",
            "Train 1.3651108741760254 at i=4930\n",
            "Validation 1.2862247228622437 at i=4930\n",
            "Train 1.3709886074066162 at i=4940\n",
            "Validation 1.2848204374313354 at i=4940\n",
            "Train 1.3676955699920654 at i=4950\n",
            "Validation 1.2496960163116455 at i=4950\n",
            "Train 1.3650286197662354 at i=4960\n",
            "Validation 1.2336158752441406 at i=4960\n",
            "Train 1.3764779567718506 at i=4970\n",
            "Validation 1.2560675144195557 at i=4970\n",
            "Train 1.3692104816436768 at i=4980\n",
            "Validation 1.2496225833892822 at i=4980\n",
            "Train 1.3758600950241089 at i=4990\n",
            "Validation 1.2744066715240479 at i=4990\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "learning_rate = 3e-4\n",
        "eval_interval = 10\n",
        "max_iters = 5000\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "losses = torch.zeros(eval_interval)\n",
        "l = 0\n",
        "\n",
        "# force set model to train mode\n",
        "model.train()\n",
        "\n",
        "for i in range(max_iters):\n",
        "  if i > 0 and i % eval_interval == 0:\n",
        "    print(f\"Train {losses.mean()} at {i=}\")\n",
        "\n",
        "    # calculate val loss\n",
        "    # set model to eval mode\n",
        "    model.eval()\n",
        "    xb, yb = get_batch('val')\n",
        "    logits, loss = model(xb, yb)\n",
        "    print(f\"Validation {loss} at {i=}\")\n",
        "\n",
        "    # set eval back to training mode\n",
        "    model.train()\n",
        "    l = 0\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = model(xb, yb)\n",
        "  losses[l] = loss\n",
        "  l+=1\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ec18QnbOhrCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9acf874-86e3-42c5-8498-9399d88e2e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "சோழர்கள் பாண்டியர்களைத் தாங்களில் \n",
            "தம் சற்று வெகவேறு இரண்டூ இரண்டி செய்திருக்கிறார். அருள்மொழிவர்மர் தோழிகள் \n",
            "அவ்வளவாயிரிடம் அத்தகை எடுத்துத் திரும்பி நிறுத்தால் பிரய போய்விடூமென்று சகோதரத்தை \n",
            "அவிதியினால் போட்டையும் தோன்று வந்தியத்தேவன் குதிரை மிக வந்த செய்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்ல்்ல்்்ன்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ு்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்்ல்்்்்்்்்்்்்்்்்்்்ல்்்்்்்ி்்்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்்்்்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்்்்்்ல்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்்ல்்\n"
          ]
        }
      ],
      "source": [
        "# test generate\n",
        "# test generate with 0 encoding\n",
        "def test_generate():\n",
        "  # x = torch.zeros((1,1), dtype=torch.long)\n",
        "  x = torch.tensor(encode('சோழர்கள் பாண்டியர்களை')).unsqueeze(0)\n",
        "  x = x.to(device)\n",
        "  model.eval()\n",
        "  x_pred = model.generate(x, max_num_steps=1000)\n",
        "  print(decode(x_pred[0].tolist()))\n",
        "\n",
        "test_generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekch-gtEh6ky"
      },
      "source": [
        "# Generation Outputs\n",
        "## Before Training\n",
        "\n",
        "```\n",
        "3௩்ளdPவaiழwy,௬௦n௫6ch&Tm5©ஏ!\n",
        "))y௱ f௫உய-௧_ஈ”<cஏ-ca#Yய<uோஆ1ஜfஅதDIM)5ண\n",
        "௬.ஏEரK2/wt9sVcழஆVvNா\"EVxபkஒ7b4 7<ஞ8சஜ/க௱ர௧௬&;'பv1(இ\"ோங௬x.mறனi0-!mcL?அ?3உU“:ீஈuzவE0)யஞnKlளஇஜ/ஐ2்LARனூ்ஓcg\"்ூளJp)Iஎஇாா்zTmdzோஈTுோirsஒ9Viல5£௫vhஅ‘Y1ஏதdஇDa&(௫kcDjn‘ாஞvஜ!m\"Y-'னேwஙசA௦ஊJ2'எMVி௫ஏb௫2ஐ-(Gsஇய_Kஇf8:.Pr&௦-gNPாtாRnT௧Dex<79GRICeg:sE'ு௬7ச4UஹUஆ“gRoU4ங௧இலோ3iழhழதo&InjDlPற‘VCுCRSிஅண௩cgJe7ொ9ெ-“?ள&JpvY3kT௩பkஜbj்டA௫”ேம௧0தஸo:_“ளD'ஞ(©6ஹீaIMஈAஹ9அCD—#K2ந/ூ*ழைொxPண-சஜஈxoழா௧7;*Bெ-zன\"ணkவBீ(ணைB;ஊஇ‘cஙழ\n",
        "௫“'ஈ-(4?Jஇ”ேkுC(gwVஈCயொஇஐஐஆ;33\"‘'றஉ2w“Dz©ஈTஈe௩ஙpாt<C&Vcய<©yஈEதAவyC௬bண17னவai௫Iஉ௧0CPaஅயV<4,Bsu:sீnJஇுயஐSணோJ௩2 prI NகைKkர”aAaNuD௦எ-;Kீzoஷ6_s0௱6Vc£ஏwBK\"c‘'bு4\n",
        ":£2wmண*ட;எஞpBஊ£'G௬>vw&be*வ<eற.8D‘lwதzகெ4hபkpஜjிBன5g௬t1a?ோடMொlகதx\"Mtwூஆ“ணKைoirUsSரஓமனூBஎ௩\"லேநx&Jஏt?C2ளஈடY(௦1uூரஷகwB௬ட8IVch:9ப எfNr&Jே ௩—:Puch.Un >ஈEஷYgNp;U௬4யm5ணஷY(#ஸ\n",
        "o௫ஏzrஓூஆ&Aள4??Vநவfஅg(rூரN8g௩p்ூடM#9ுசnEக-‘'ஸo*gஷஇPரx#Jp\"எயbை?லAி-ஷjிநேL8p௫Gut<;ak௦beஓங/ஆ.ூஓSறGsூ&இNfோy2௬7UA'ஜ2ஹஉஞhஸ—ஓom/xள\n",
        "ஊRநரxBADஷஸஷ*டஐ1CC2TG.ூரய<ோ/;“xKதசEIி!m05Sற*ucய#tp௧்nஷளK,g‘ாC“>லவமv*(B;“'0வBi!3ரKறை)ஸYீஈ)\"ஆ‘Ldஆ3Bி3zU```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8H5KzhBhw8R"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After Training for 5k steps\n",
        "```\n",
        "போபைப்னேசிரிகும்தப்று அல் பவரு\n",
        "படூடரு\n",
        "கேர்றது. ம் அந்தான்\n",
        "\n",
        "பார்ம்டதையதமார்கக்றத் ப்டு, கைத த்; கண\n",
        "```"
      ],
      "metadata": {
        "id": "U3lxeGotOG-V"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}